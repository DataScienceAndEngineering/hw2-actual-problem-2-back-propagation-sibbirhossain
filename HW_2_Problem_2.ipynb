{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. **Proof for Equations (BP3)**\n",
        "\n",
        "Equation to prove: ∂C/∂blj = δlj\n",
        "\n"
      ],
      "metadata": {
        "id": "lhtzsZFxEneT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proof:**\n",
        "\n",
        "In backpropagation, the error term δlj at the jth neuron in layer l is defined as:"
      ],
      "metadata": {
        "id": "Hz1H4_LJMHBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "δLj = ∂C / ∂zlj σ′(zlj)"
      ],
      "metadata": {
        "id": "wBrUrn4UMcHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "∂C / ∂zlj represents the rate of change of the cost with respect to the weighted input zlj.\n",
        "\n",
        "σ′(zlj) is the derivative of the activation function with respect to zlj ."
      ],
      "metadata": {
        "id": "xROcEuwbMda0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take the partial derivative of the cost function with respect to the bias term blj in layer l:\n"
      ],
      "metadata": {
        "id": "g2I1U5NBM0XD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch6BuyYXEfae"
      },
      "outputs": [],
      "source": [
        "∂C/∂blj = ∂C/ ∂zlj * ∂zlj/∂blj"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second term, ∂zlj /∂blj, represents the derivative of the weighted input zlj with respect to the bias blj.\n",
        "\n",
        "Since zlj is a linear combination of weights and biases, derivative is 1.\n"
      ],
      "metadata": {
        "id": "dCxbfQ0gM_LO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Substitute ∂C/∂zlj for ∂C/∂blj :"
      ],
      "metadata": {
        "id": "oxg07ow9NLFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "∂C/∂blj = ∂C/∂zlj\n"
      ],
      "metadata": {
        "id": "ZDiqM7vCM-xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the partial derivative of the cost with respect to the bias term is the same as the error term at the corresponding neuron:"
      ],
      "metadata": {
        "id": "d_gKQvx5NOpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "∂C / ∂blj = δlj"
      ],
      "metadata": {
        "id": "tAPb2VxeNRJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Proof for Equations (BP4)**\n",
        "\n",
        "Equation to prove: ∂C / ∂wljk = al-1k δlj"
      ],
      "metadata": {
        "id": "8_y_tMTlNe6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proof:\n",
        "\n",
        "In backpropagation, the error term δlj at the jth neuron in layer l is defined as:"
      ],
      "metadata": {
        "id": "GxggHc3bQO3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "δLj = ∂C / ∂zlj σ′(zlj)"
      ],
      "metadata": {
        "id": "Hz_RDJcGQRbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "∂C / ∂zlj represents the rate of change of the cost with respect to the weighted input zlj.\n",
        "\n",
        "σ′(zlj) is the derivative of the activation function with respect to zlj ."
      ],
      "metadata": {
        "id": "7sfFxW6zQXd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can express zlj in Terms of Weights and Biases"
      ],
      "metadata": {
        "id": "Eg3BDiMZQnGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zlj = ∑k wljk al−1k + blj\n"
      ],
      "metadata": {
        "id": "T_FFCPYeQqbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This expression represents the weighted sum of inputs from the neurons in layer l−1, plus the bias."
      ],
      "metadata": {
        "id": "bAckYlZ0Qsis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take the partial derivative of the cost with respect to the weight wljk :"
      ],
      "metadata": {
        "id": "mvK6e7IeQ1LG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "∂C / ∂wljk  = ∂ / ∂wljk (1/2 (alj − yj )^2)"
      ],
      "metadata": {
        "id": "dv3dwbR_Q23N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the chain rule, this expression can be expanded as:"
      ],
      "metadata": {
        "id": "3SBKJ0pSQ6Da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "∂C / ∂wljk  =(alj −yj ) ∂alj / ∂wljk"
      ],
      "metadata": {
        "id": "MEf_9TboQ7kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The term ∂alj / ∂wljk   involves the input to the activation function at the jth neuron in layer l:\n"
      ],
      "metadata": {
        "id": "_DKNmWCNQ9i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "∂alj / ∂wljk   =al−1k"
      ],
      "metadata": {
        "id": "z5vdkaaXQ-8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Substituting back into the original expression"
      ],
      "metadata": {
        "id": "fLslXRi_RCAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "∂C / ∂wljk = (alj −yj ) al−1k"
      ],
      "metadata": {
        "id": "Xh-5-n19RBo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "alj − yj is often represented as δlj , the error term at the jth neuron in layer l:"
      ],
      "metadata": {
        "id": "IK2qu29XRGuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "∂C / ∂wljk  = δlj al−1k\n"
      ],
      "metadata": {
        "id": "r6nLAGfQRHUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. The fully matrix-based approach to backpropagation across a mini-batch involves conducting operations on entire matrices of data rather than iterating through individual training examples as in the loop-based method. This method utilizes vectorized operations to process the entire mini-batch simultaneously, enhancing efficiency and speed."
      ],
      "metadata": {
        "id": "GHqTFSqeUAom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Iris Dataset: load the iris dataset and add a constant column x0 = 1 to input features."
      ],
      "metadata": {
        "id": "PtBoe9XXYcki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "rYtCsaEnYhWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "metadata": {
        "id": "Fdzgf5NyYcWe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a constant column to X\n",
        "X = np.column_stack((np.ones(X.shape[0]), X))"
      ],
      "metadata": {
        "id": "kkGqvXMpYnTR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to one-hot encoding\n",
        "y_one_hot = np.eye(3)[y]\n"
      ],
      "metadata": {
        "id": "GjiC0clUYpfX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "-wOa3wjmYr8s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modify Neural Network Class\n"
      ],
      "metadata": {
        "id": "w2IlM95cX69w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The update_mini_batch method is where the backpropagation algorithm is implemented.\n",
        "\n",
        "In the matrix-based approach, the input mini-batch is transposed to facilitate matrix multiplication. Assume that the input variables are augmented with a \"column\" of \"1\"s, and the weights w0."
      ],
      "metadata": {
        "id": "wd5QrZHFX53c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Network(object):\n",
        "     def __init__(self, sizes):\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "\n",
        "     def update_mini_batch_matrix(self, mini_batch, eta):\n",
        "        #Update the network's weights using matrix-based backpropagation for a single mini batch.\n",
        "        mini_batch_size = len(mini_batch)\n",
        "\n",
        "        # Extract features (inputs) and labels (outputs) from the mini-batch\n",
        "        X = np.array([x for x, y in mini_batch]).T\n",
        "        Y = np.array([y for x, y in mini_batch]).T\n",
        "\n",
        "        # Forward propagation\n",
        "        Z = np.dot(self.weights, X)\n",
        "        A = sigmoid(Z)\n",
        "\n",
        "        # Backward propagation\n",
        "        delta = self.cost_derivative(A, Y) * sigmoid_prime(Z)\n",
        "        nabla_w = np.dot(delta, X.T)\n",
        "\n",
        "        # Update weights\n",
        "        self.weights -= (eta / mini_batch_size) * nabla_w\n",
        "\n",
        "     def backprop(self, x, y):\n",
        "        #Return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function C_x.  \"nabla_b\" and\n",
        "        #\"nabla_w\" are layer-by-layer lists of numpy arrays, similar to \"self.biases\" and \"self.weights\".\"\"\"\n",
        "        #nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation)+b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * \\\n",
        "            sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "        for l in xrange(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "     def cost_derivative(self, output_activations, y):\n",
        "        #Return the vector of partial derivatives\n",
        "        return (output_activations - y)\n",
        "\n",
        "      # Define sigmoid and sigmoid_prime functions\n",
        "     def sigmoid(z):\n",
        "        return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "     def sigmoid_prime(z):\n",
        "        return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "training_data = [(np.random.randn(3, 1), np.random.randn(2, 1)) for _ in range(100)]"
      ],
      "metadata": {
        "id": "ihC80elWD9tm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function update_mini_batch_matrix receives a mini-batch and a learning rate (eta) as parameters. It proceeds by extracting features (inputs) X and labels (outputs) Y from the mini-batch. Then, it conducts forward propagation using matrix multiplication. Afterward, it calculates the derivative of the cost function concerning the activations (delta) and the gradient of the weights (nabla_w) utilizing matrix operations. Finally, it updates the weights using the computed gradient. This method enhances efficiency by leveraging matrix operations, thereby enhancing the speed of the backpropagation algorithm."
      ],
      "metadata": {
        "id": "m8txq95DFi0l"
      }
    }
  ]
}